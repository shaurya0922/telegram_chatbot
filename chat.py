import re 

import sys 

import io 

import contextlib 

import traceback 

import ast 

import importlib 

import importlib.util 

from typing import Dict, Any, Tuple, Optional, List, Set, Union 

import requests

import json 

import time 

import subprocess 

import os 

from dataclasses import dataclass, field 

from datetime import datetime 

import hashlib 

from collections import defaultdict  

from dotenv import load_dotenv 

import threading 

import queue 

import copy 

os.environ["GEMINI_API_KEY"] = "AIzaSyCYRMZbUhGCoqDpwCwcluGM6srNqUc_zzY"

def call_gemini(prompt, model="gemini-1.5-flash", temperature=0.7, max_output_tokens=512):
    api_key = os.getenv("GEMINI_API_KEY")
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}"

    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {
            "temperature": temperature,
            "maxOutputTokens": max_output_tokens
        }
    }

    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        data = response.json()
        return data["candidates"][0]["content"]["parts"][0]["text"]
    else:
        raise Exception(f"Gemini API Error {response.status_code}: {response.text}")
    

load_dotenv()

@dataclass 

class ExecutionState: 

    """Tracks the state of code execution""" 

    variables: Dict[str, Any] = field(default_factory=dict) 

    imports: Set[str] = field(default_factory=set) 

    functions: Dict[str, str] = field(default_factory=dict) 

    data_artifacts: Dict[str, Any] = field(default_factory=dict) 

    exploration_results: Dict[str, Any] = field(default_factory=dict) 

@dataclass 

class GoalNode: 

    """Represents a goal in the goal decomposition tree""" 

    id: str 

    description: str 

    parent_id: Optional[str] = None 

    children: List[str] = field(default_factory=list) 

    status: str = "pending"  # pending, in_progress, completed, failed, blocked 

    priority: int = 1 

    dependencies: List[str] = field(default_factory=list) 

    result: Optional[str] = None 

    reasoning_chain: List[str] = field(default_factory=list) 

    execution_context: Dict[str, Any] = field(default_factory=dict) 

    retry_count: int = 0 

    max_retries: int = 5 

    complexity_score: float = 1.0 

@dataclass 

class AutonomousMemory: 

    """Enhanced memory for smarter autonomous learning""" 

    execution_patterns: List[Dict[str, Any]] = field(default_factory=list) 

    successful_strategies: List[Dict[str, Any]] = field(default_factory=list) 

    installed_packages: Set[str] = field(default_factory=set) 

    context_discovered: Dict[str, Any] = field(default_factory=dict) 

    error_recovery_strategies: List[Dict[str, Any]] = field(default_factory=list) 

    goal_tree: Dict[str, GoalNode] = field(default_factory=dict) 

    reasoning_chains: List[List[str]] = field(default_factory=list) 

    exploration_cache: Dict[str, Any] = field(default_factory=dict) 

    execution_state: ExecutionState = field(default_factory=ExecutionState) 

    meta_learning: Dict[str, Any] = field(default_factory=dict) 

     

class ComplexityAnalyzer: 

    """Analyzes query complexity and suggests decomposition strategies""" 

@staticmethod 

def analyze_complexity(query: str) -> Dict[str, Any]: 

        """Analyze query complexity and return insights""" 

        complexity_indicators = { 

            'data_operations': ['analyze', 'process', 'clean', 'transform', 'merge', 'join', 'aggregate'], 

            'visualization': ['plot', 'chart', 'graph', 'visualize', 'show', 'display'], 

            'machine_learning': ['predict', 'classify', 'cluster', 'train', 'model', 'accuracy'], 

            'web_scraping': ['scrape', 'crawl', 'extract', 'website', 'url', 'web'], 

            'file_operations': ['file', 'csv', 'json', 'excel', 'read', 'write', 'save'], 

            'api_integration': ['api', 'request', 'endpoint', 'fetch', 'retrieve'], 

            'multi_step': ['and', 'then', 'after', 'next', 'finally', 'also'], 

            'comparative': ['compare', 'versus', 'vs', 'difference', 'contrast'], 

            'comprehensive': ['comprehensive', 'detailed', 'thorough', 'complete', 'full'] 

        } 

         

        query_lower = query.lower() 

        detected_types = [] 

        complexity_score = 1.0 

         

        for category, keywords in complexity_indicators.items(): 

            if any(keyword in query_lower for keyword in keywords): 

                detected_types.append(category) 

                complexity_score += 0.5 
# Additional complexity factors 

        if len(query.split()) > 20: 

            complexity_score += 0.3 

        if '?' in query and query.count('?') > 1: 

            complexity_score += 0.2 

        if any(word in query_lower for word in ['multiple', 'several', 'various', 'different']): 

            complexity_score += 0.4 

             

        return { 

            'complexity_score': min(complexity_score, 5.0), 

            'detected_types': detected_types, 

            'is_complex': complexity_score > 2.0, 

            'suggested_approach': ComplexityAnalyzer._suggest_approach(detected_types, complexity_score) 

        } 

@staticmethod 

def _suggest_approach(detected_types: List[str], complexity_score: float) -> str: 

        """Suggest the best approach based on complexity analysis""" 

        if complexity_score > 3.5: 

            return "hierarchical_decomposition" 

        elif len(detected_types) > 3: 

            return "parallel_execution" 

        elif 'data_operations' in detected_types and len(detected_types) > 1: 

            return "pipeline_approach" 

        else: 

            return "iterative_refinement" 

 

class TrulyAutonomousAgent: 

    """ 

    Enhanced TRULY autonomous agent with advanced capabilities for complex queries 

    """ 

     

    def __init__(self): 

        # Core components 

        self.execution_context = {} 

        self.session_history = [] 

        self.memory = AutonomousMemory() 

        self.max_iterations = 25 

        self.complexity_analyzer = ComplexityAnalyzer() 

         

        # Enhanced execution tracking 

        self.global_execution_state = ExecutionState() 

        self.exploration_depth = 3 

        self.adaptive_reasoning = True 

class LLMConfig:
    def __init__(self):
        # LLM configuration
        self.gemini_api_key = os.getenv("AIzaSyCYRMZbUhGCoqDpwCwcluGM6srNqUc_zzY")
        self.model_name = "gemini-1.5-flash"
        self.active_llm = "gemini"

        # Enhanced prompting strategies
        self.reasoning_strategies = {
            'exploratory': self._create_exploratory_prompt,
            'analytical': self._create_analytical_prompt,
            'synthetic': self._create_synthetic_prompt,
            'adaptive': self._create_adaptive_prompt
        }

    # Example strategy prompts
    def _create_exploratory_prompt(self):
        return "Explain this in a simple and exploratory way."

    def _create_analytical_prompt(self):
        return "Provide a detailed, step-by-step analytical explanation."

    def _create_synthetic_prompt(self):
        return "Summarize and synthesize the key insights."

    def _create_adaptive_prompt(self):
        return "Adapt your response based on the user's context and style."

    # ðŸ”¹ Main chat method
    def chat(self, user_input: str, strategy="exploratory"):
        if strategy not in self.reasoning_strategies:
            raise ValueError(f"Unknown strategy: {strategy}")

        base_prompt = self.reasoning_strategies[strategy]()
        final_prompt = f"{base_prompt}\n\nUser: {user_input}"

        return call_gemini(final_prompt, model=self.model_name)

if __name__ == "__main__":
    llm = LLMConfig()
    response = llm.chat("What are microwaves?", strategy="exploratory")
    print("Bot:", response)
